{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f03b517",
   "metadata": {},
   "outputs": [],
   "source": [
    "#... Taking from the previous script - check previous functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e72f0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiments\n",
    "for seed_idx, seed in enumerate(seeds):\n",
    "    for exp in experiments:\n",
    "        all_metrics_before = []\n",
    "        all_metrics_after = []\n",
    "        all_fairness_before = []\n",
    "        all_fairness_after = []\n",
    "        all_bias_diagnosis = []\n",
    "        all_feature_importance = []  # Store feature importance for each flip\n",
    "        \n",
    "        # DataFrames to store results for this seed\n",
    "        metrics_before_df = []\n",
    "        metrics_after_df = []\n",
    "        fairness_before_df = []\n",
    "        fairness_after_df = []\n",
    "        bias_diagnosis_df = []\n",
    "        \n",
    "        for flip_pct in flip_percentages:\n",
    "            np.random.seed(seed)\n",
    "            mydf2 = mydf.copy()\n",
    "            mydf2['statusflip'] = mydf2['StatusLabel'].copy()\n",
    "            mask = (mydf2['Gender'] == 'Female') & (mydf2['statusflip'] == 'Not Depressed')\n",
    "            if mask.sum() > 0:\n",
    "                mydf2.loc[mask, 'statusflip'] = np.random.choice(\n",
    "                    ['Not Depressed', 'Depressed'],\n",
    "                    size=mask.sum(),\n",
    "                    p=[1 - flip_pct, flip_pct]\n",
    "                )\n",
    "            mydf2['statusflipbin'] = mydf2['statusflip'].map({'Not Depressed': 0, 'Depressed': 1})\n",
    "            ybias = mydf2['statusflipbin']\n",
    "            \n",
    "            X_temp, X_test, ytrue_temp, ytrue_test, ybias_temp, ybias_test, sensitive_temp, sensitive_test = train_test_split(\n",
    "                X, ytrue, ybias, sensitive_feature, test_size=0.2, random_state=seed, stratify=ytrue\n",
    "            )\n",
    "            X_train, X_val, ytrue_train, ytrue_val, ybias_train, ybias_val, sensitive_train, sensitive_val = train_test_split(\n",
    "                X_temp, ytrue_temp, ybias_temp, sensitive_temp, test_size=0.25, random_state=seed, stratify=ytrue_temp\n",
    "            )\n",
    "            \n",
    "            y_train = ytrue_train if exp['train'] == 'true' else ybias_train\n",
    "            y_val_for_mitigation = ytrue_val if exp['train'] == 'true' else ybias_val\n",
    "            y_test = ytrue_test if exp['test'] == 'true' else ybias_test\n",
    "            \n",
    "            ####################################################################\n",
    "            # Use CatBoostClassifier with scale_pos_weight for imbalance\n",
    "            gbm = CatBoostClassifier(random_seed=seed, scale_pos_weight=np.sum(y_train == 0) / np.sum(y_train == 1), verbose=0)\n",
    "            gbm.fit(X_train, y_train)\n",
    "            y_pred = gbm.predict(X_test)\n",
    "            y_pred_proba = gbm.predict_proba(X_test)\n",
    "            y_pred_mitigated = apply_mitigation(gbm, X_val, y_pred_proba, sensitive_test, y_val_for_mitigation, sensitive_val)\n",
    "\n",
    "            ####################################################################\n",
    "            # Store feature importance for this flip percentage\n",
    "            importance = gbm.get_feature_importance()  # Use feature importances from CatBoostClassifier\n",
    "            all_feature_importance.append(importance)\n",
    "            ####################################################################\n",
    "\n",
    "            male_mask = (sensitive_test == 'Male')\n",
    "            female_mask = (sensitive_test == 'Female')\n",
    "            overall_metrics = compute_metrics(y_test, y_pred, group_name='Overall')\n",
    "            male_metrics = compute_metrics(y_test[male_mask], y_pred[male_mask], group_name='Male')\n",
    "            female_metrics = compute_metrics(y_test[female_mask], y_pred[female_mask], group_name='Female')\n",
    "            fairness = fairness_diagnosis(male_metrics, female_metrics)\n",
    "            \n",
    "            overall_metrics_mitigated = compute_metrics(y_test, y_pred_mitigated, group_name='Overall')\n",
    "            male_metrics_mitigated = compute_metrics(y_test[male_mask], y_pred_mitigated[male_mask], group_name='Male')\n",
    "            female_metrics_mitigated = compute_metrics(y_test[female_mask], y_pred_mitigated[female_mask], group_name='Female')\n",
    "            fairness_mitigated = fairness_diagnosis(male_metrics_mitigated, female_metrics_mitigated)\n",
    "            \n",
    "            bias_diag = bias_diagnosis(ytrue_test, ybias_test, sensitive_test)\n",
    "            \n",
    "            all_metrics_before.append({'Overall': overall_metrics, 'Male': male_metrics, 'Female': female_metrics})\n",
    "            all_metrics_after.append({'Overall': overall_metrics_mitigated, 'Male': male_metrics_mitigated, 'Female': female_metrics_mitigated})\n",
    "            all_fairness_before.append(fairness)\n",
    "            all_fairness_after.append(fairness_mitigated)\n",
    "            all_bias_diagnosis.append(bias_diag)\n",
    "            \n",
    "            # Store results for this seed in DataFrames\n",
    "            for group in ['Male', 'Female']:\n",
    "                metrics_before_df.append({\n",
    "                    'Flip Percentage': flip_pct * 100,\n",
    "                    'Group': group,\n",
    "                    **{metric: all_metrics_before[-1][group][metric] for metric in metric_names}\n",
    "                })\n",
    "                metrics_after_df.append({\n",
    "                    'Flip Percentage': flip_pct * 100,\n",
    "                    'Group': group,\n",
    "                    **{metric: all_metrics_after[-1][group][metric] for metric in metric_names}\n",
    "                })\n",
    "            fairness_before_df.append({\n",
    "                'Flip Percentage': flip_pct * 100,\n",
    "                **{row['Metric']: row['Value'] for _, row in fairness.iterrows()}\n",
    "            })\n",
    "            fairness_after_df.append({\n",
    "                'Flip Percentage': flip_pct * 100,\n",
    "                **{row['Metric']: row['Value'] for _, row in fairness_mitigated.iterrows()}\n",
    "            })\n",
    "            bias_diagnosis_df.append({\n",
    "                'Flip Percentage': flip_pct * 100,\n",
    "                'Male Error Rate': bias_diag['Male'],\n",
    "                'Female Error Rate': bias_diag['Female']\n",
    "            })\n",
    "        \n",
    "        all_experiment_results[exp['name']]['metrics_before'][seed_idx] = all_metrics_before\n",
    "        all_experiment_results[exp['name']]['metrics_after'][seed_idx] = all_metrics_after\n",
    "        all_experiment_results[exp['name']]['fairness_before'][seed_idx] = all_fairness_before\n",
    "        all_experiment_results[exp['name']]['fairness_after'][seed_idx] = all_fairness_after\n",
    "        all_experiment_results[exp['name']]['bias_diagnosis'][seed_idx] = all_bias_diagnosis\n",
    "        all_experiment_results[exp['name']]['feature_importance'][seed_idx] = all_feature_importance\n",
    "        \n",
    "        # Save seed-specific results to CSVs\n",
    "        pd.DataFrame(metrics_before_df).to_csv(f\"metrics_before_{exp['name']}_seed_{seed}.csv\", index=False)\n",
    "        pd.DataFrame(metrics_after_df).to_csv(f\"metrics_after_{exp['name']}_seed_{seed}.csv\", index=False)\n",
    "        pd.DataFrame(fairness_before_df).to_csv(f\"fairness_before_{exp['name']}_seed_{seed}.csv\", index=False)\n",
    "        pd.DataFrame(fairness_after_df).to_csv(f\"fairness_after_{exp['name']}_seed_{seed}.csv\", index=False)\n",
    "        pd.DataFrame(bias_diagnosis_df).to_csv(f\"bias_diagnosis_{exp['name']}_seed_{seed}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6897935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate mean and standard error\n",
    "def calculate_stats(data_list, metric, group=None):\n",
    "    if group:\n",
    "        values = np.array([[m[group][metric] for m in metrics] for metrics in data_list])\n",
    "    else:\n",
    "        values = np.array([[df[df['Metric'] == metric]['Value'].values[0] for df in dfs] for dfs in data_list])\n",
    "    mean_values = np.mean(values, axis=0)\n",
    "    se_values = sem(values, axis=0)\n",
    "    return mean_values, se_values\n",
    "\n",
    "# Generate plots, print mean and SE, and save mean/SE CSVs\n",
    "for exp in experiments:\n",
    "    exp_name = exp['name']\n",
    "    results = all_experiment_results[exp_name]\n",
    "    \n",
    "    # Plot VIP across seeds for this experiment\n",
    "    plot_vip_across_seeds(results['feature_importance'], flip_percentages, feature_names, exp_name)\n",
    "    \n",
    "    # DataFrames for mean and SE\n",
    "    metrics_mean_se_df = []\n",
    "    fairness_mean_se_df = []\n",
    "    bias_diagnosis_mean_se_df = []\n",
    "    \n",
    "    # Print mean and SE for standard metrics and store in DataFrame\n",
    "    for group in ['Male', 'Female']:\n",
    "        for metric in metric_names:\n",
    "            mean_values_before, se_values_before = calculate_stats(results['metrics_before'], metric, group)\n",
    "            mean_values_after, se_values_after = calculate_stats(results['metrics_after'], metric, group)\n",
    "            print(f\"{exp_name} {group} {metric} - Before Mean: {mean_values_before}, SE: {se_values_before}\")\n",
    "            print(f\"{exp_name} {group} {metric} - After Mean: {mean_values_after}, SE: {se_values_after}\")\n",
    "            \n",
    "            for i, flip_pct in enumerate(flip_percentages):\n",
    "                metrics_mean_se_df.append({\n",
    "                    'Flip Percentage': flip_pct * 100,\n",
    "                    'Group': group,\n",
    "                    'Metric': metric,\n",
    "                    'Before Mean': mean_values_before[i],\n",
    "                    'Before SE': se_values_before[i],\n",
    "                    'After Mean': mean_values_after[i],\n",
    "                    'After SE': se_values_after[i]\n",
    "                })\n",
    "    \n",
    "    # Plot metrics (with overall line and mean ± SE annotations)\n",
    "    plot_metrics_across_flips(results['metrics_before'], [p*100 for p in flip_percentages], metric_names,\n",
    "                             f\"{exp_name} - Before Mitigation\", f\"metrics_vs_flip_{exp_name}_before.pdf\")\n",
    "    plot_metrics_across_flips(results['metrics_after'], [p*100 for p in flip_percentages], metric_names,\n",
    "                             f\"{exp_name} - After Mitigation\", f\"metrics_vs_flip_{exp_name}_after.pdf\")\n",
    "    \n",
    "    # Fairness metrics plot (with mean ± SE annotations)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for i, metric in enumerate(fairness_metrics):\n",
    "        before_values = np.array([[df[df['Metric'] == metric]['Value'].values[0] for df in dfs] for dfs in results['fairness_before']])\n",
    "        after_values = np.array([[df[df['Metric'] == metric]['Value'].values[0] for df in dfs] for dfs in results['fairness_after']])\n",
    "        before_mean = np.mean(before_values, axis=0)\n",
    "        after_mean = np.mean(after_values, axis=0)\n",
    "        before_se = sem(before_values, axis=0)\n",
    "        after_se = sem(after_values, axis=0)\n",
    "        \n",
    "        # Calculate overall mean and SE across all flip percentages\n",
    "        overall_before_mean = np.mean(before_mean)\n",
    "        overall_before_se = np.mean(before_se)\n",
    "        overall_after_mean = np.mean(after_mean)\n",
    "        overall_after_se = np.mean(after_se)\n",
    "        \n",
    "        print(f\"{exp_name} Fairness {metric} - Before Mean: {before_mean}, SE: {before_se}\")\n",
    "        print(f\"{exp_name} Fairness {metric} - After Mean: {after_mean}, SE: {after_se}\")\n",
    "        \n",
    "        for j, flip_pct in enumerate(flip_percentages):\n",
    "            fairness_mean_se_df.append({\n",
    "                'Flip Percentage': flip_pct * 100,\n",
    "                'Metric': metric,\n",
    "                'Before Mean': before_mean[j],\n",
    "                'Before SE': before_se[j],\n",
    "                'After Mean': after_mean[j],\n",
    "                'After SE': after_se[j]\n",
    "            })\n",
    "        \n",
    "        plt.subplot(2, 2, i+1)\n",
    "        plt.plot([p*100 for p in flip_percentages], before_mean, 'o-', color='#1f77b4', label='Before Mean')\n",
    "        plt.fill_between([p*100 for p in flip_percentages], before_mean - before_se, before_mean + before_se, color='#1f77b4', alpha=0.2, label='Before Mean ± SE')\n",
    "        plt.plot([p*100 for p in flip_percentages], after_mean, 'o-', color='#ff7f0e', label='After Mean')\n",
    "        plt.fill_between([p*100 for p in flip_percentages], after_mean - after_se, after_mean + after_se, color='#ff7f0e', alpha=0.2, label='After Mean ± SE')\n",
    "        \n",
    "        # Add overall mean ± SE as text annotation\n",
    "        plt.text(0.02, 0.98, f'Before: {overall_before_mean:.4f} ± {overall_before_se:.4f}\\nAfter: {overall_after_mean:.4f} ± {overall_after_se:.4f}',\n",
    "                 transform=plt.gca().transAxes, fontsize=8, verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "        \n",
    "        plt.xlabel('Flip Percentage')\n",
    "        plt.ylabel(metric)\n",
    "        plt.title(f'{metric} vs Flip Percentage')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.ylim(0.0, 1.0)  # Set y-axis limits to 0.0 to 1.0\n",
    "    \n",
    "    plt.suptitle(f'Fairness Metrics vs Flip Percentage ({exp_name})', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    with PdfPages(f\"fairness_vs_flip_{exp_name}.pdf\") as pdf:\n",
    "        pdf.savefig()\n",
    "    plt.close()\n",
    "    \n",
    "    # Bias diagnosis plot (with overall line and mean ± SE annotations)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    male_bias = np.array([[bias['Male'] for bias in biases] for biases in results['bias_diagnosis']])\n",
    "    female_bias = np.array([[bias['Female'] for bias in biases] for biases in results['bias_diagnosis']])\n",
    "    male_mean = np.mean(male_bias, axis=0)\n",
    "    female_mean = np.mean(female_bias, axis=0)\n",
    "    male_se = sem(male_bias, axis=0)\n",
    "    female_se = sem(female_bias, axis=0)\n",
    "    \n",
    "    # Calculate overall bias (mean of Male and Female error rates)\n",
    "    overall_bias = (male_bias + female_bias) / 2\n",
    "    overall_mean = np.mean(overall_bias, axis=0)\n",
    "    overall_se = sem(overall_bias, axis=0)\n",
    "    \n",
    "    # Calculate overall mean and SE across all flip percentages\n",
    "    overall_male_mean = np.mean(male_mean)\n",
    "    overall_male_se = np.mean(male_se)\n",
    "    overall_female_mean = np.mean(female_mean)\n",
    "    overall_female_se = np.mean(female_se)\n",
    "    overall_overall_mean = np.mean(overall_mean)\n",
    "    overall_overall_se = np.mean(overall_se)\n",
    "    \n",
    "    print(f\"{exp_name} Bias Diagnosis - Male Mean: {male_mean}, SE: {male_se}\")\n",
    "    print(f\"{exp_name} Bias Diagnosis - Female Mean: {female_mean}, SE: {female_se}\")\n",
    "    print(f\"{exp_name} Bias Diagnosis - Overall Mean: {overall_mean}, SE: {overall_se}\")\n",
    "    \n",
    "    for i, flip_pct in enumerate(flip_percentages):\n",
    "        bias_diagnosis_mean_se_df.append({\n",
    "            'Flip Percentage': flip_pct * 100,\n",
    "            'Male Mean': male_mean[i],\n",
    "            'Male SE': male_se[i],\n",
    "            'Female Mean': female_mean[i],\n",
    "            'Female SE': female_se[i],\n",
    "            'Overall Mean': overall_mean[i],\n",
    "            'Overall SE': overall_se[i]\n",
    "        })\n",
    "    \n",
    "    plt.plot([p*100 for p in flip_percentages], male_mean, 'o-', color='#1f77b4', label='Male Mean')\n",
    "    plt.fill_between([p*100 for p in flip_percentages], male_mean - male_se, male_mean + male_se, color='#1f77b4', alpha=0.2, label='Male Mean ± SE')\n",
    "    plt.plot([p*100 for p in flip_percentages], female_mean, 'o-', color='#ff7f0e', label='Female Mean')\n",
    "    plt.fill_between([p*100 for p in flip_percentages], female_mean - female_se, female_mean + female_se, color='#ff7f0e', alpha=0.2, label='Female Mean ± SE')\n",
    "    plt.plot([p*100 for p in flip_percentages], overall_mean, 'o-', color='#2ca02c', label='Overall Mean')\n",
    "    plt.fill_between([p*100 for p in flip_percentages], overall_mean - overall_se, overall_mean + overall_se, color='#2ca02c', alpha=0.2, label='Overall Mean ± SE')\n",
    "    \n",
    "    # Add overall mean ± SE as text annotation\n",
    "    plt.text(0.02, 0.98, f'Male: {overall_male_mean:.4f} ± {overall_male_se:.4f}\\nFemale: {overall_female_mean:.4f} ± {overall_female_se:.4f}\\nOverall: {overall_overall_mean:.4f} ± {overall_overall_se:.4f}',\n",
    "             transform=plt.gca().transAxes, fontsize=8, verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.xlabel('Flip Percentage')\n",
    "    plt.ylabel('Bias Error Rate')\n",
    "    plt.title(f'Bias Error Rate vs Flip Percentage ({exp_name})')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.ylim(0.0, 1.0)  # Set y-axis limits to 0.0 to 1.0\n",
    "    plt.tight_layout()\n",
    "    with PdfPages(f\"bias_error_vs_flip_{exp_name}.pdf\") as pdf:\n",
    "        pdf.savefig()\n",
    "    plt.close()\n",
    "    \n",
    "    # Save mean and SE results to CSVs\n",
    "    pd.DataFrame(metrics_mean_se_df).to_csv(f\"metrics_mean_se_{exp_name}.csv\", index=False)\n",
    "    pd.DataFrame(fairness_mean_se_df).to_csv(f\"fairness_mean_se_{exp_name}.csv\", index=False)\n",
    "    pd.DataFrame(bias_diagnosis_mean_se_df).to_csv(f\"bias_diagnosis_mean_se_{exp_name}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
