{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c672fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from lightgbm import LGBMClassifier  # Changed to LGBMClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from scipy.stats import sem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c727ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define seeds and flip percentages\n",
    "seeds = [44, 440, 200, 1000, 300, 430, 5490, 234, 456, 564]\n",
    "flip_percentages = [i/100 for i in range(0, 71, 5)]\n",
    "metric_names = ['Accuracy', 'Selection rate', 'TPR', 'TNR', 'FPR', 'FNR', 'PPV', 'NPV', 'ROC AUC', 'F1']\n",
    "fairness_metrics = ['Independence (M_ind)', 'Separation (M_sep)', 'Sufficiency (M_suff)']\n",
    "\n",
    "# Define experiments\n",
    "experiments = [\n",
    "    {'train': 'true', 'test': 'true', 'name': 'True-True'},\n",
    "    {'train': 'true', 'test': 'bias', 'name': 'True-Bias'},\n",
    "    {'train': 'bias', 'test': 'true', 'name': 'Bias-True'},\n",
    "    {'train': 'bias', 'test': 'bias', 'name': 'Bias-Bias'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e4ee4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize results dictionary\n",
    "all_experiment_results = {\n",
    "    exp['name']: {\n",
    "        'metrics_before': [[] for _ in seeds],\n",
    "        'metrics_after': [[] for _ in seeds],\n",
    "        'fairness_before': [[] for _ in seeds],\n",
    "        'fairness_after': [[] for _ in seeds],\n",
    "        'bias_diagnosis': [[] for _ in seeds],\n",
    "        'feature_importance': [[] for _ in seeds]  # Added for feature importance storage\n",
    "    } for exp in experiments\n",
    "}\n",
    "\n",
    "# Function to compute metrics\n",
    "def compute_metrics(y_true, y_pred, group_name='Overall'):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    sel_rate = np.mean(y_pred)\n",
    "    tpr = recall_score(y_true, y_pred, zero_division=0)\n",
    "    ppv = precision_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    roc_auc = roc_auc_score(y_true, y_pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    tnr = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    fpr = fp / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    fnr = fn / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "    return {\n",
    "        'Group': group_name,\n",
    "        'Accuracy': round(acc, 4),\n",
    "        'Selection rate': round(sel_rate, 4),\n",
    "        'TPR': round(tpr, 4),\n",
    "        'TNR': round(tnr, 4),\n",
    "        'FPR': round(fpr, 4),\n",
    "        'FNR': round(fnr, 4),\n",
    "        'PPV': round(ppv, 4),\n",
    "        'NPV': round(npv, 4),\n",
    "        'ROC AUC': round(roc_auc, 4),\n",
    "        'F1': round(f1, 4)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3915e1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function for fairness diagnosis\n",
    "def fairness_diagnosis(metrics_male, metrics_female):\n",
    "    groups = ['Male', 'Female']\n",
    "    metrics_list = [metrics_male, metrics_female]\n",
    "    sr_diffs = [max(abs(g1['Selection rate'] - g2['Selection rate']) for j, g2 in enumerate(metrics_list) if i != j) for i, g1 in enumerate(metrics_list)]\n",
    "    m_ind = max(sr_diffs)\n",
    "    sep_scores = [max(abs(g1['TPR'] - g2['TPR']) + abs(g1['FPR'] - g2['FPR']) for j, g2 in enumerate(metrics_list) if i != j) for i, g1 in enumerate(metrics_list)]\n",
    "    m_sep = max(sep_scores)\n",
    "    suff_scores = [max(abs(g1['PPV'] - g2['PPV']) + abs(g1['NPV'] - g2['NPV']) for j, g2 in enumerate(metrics_list) if i != j) for i, g1 in enumerate(metrics_list)]\n",
    "    m_suff = max(suff_scores)\n",
    "    return pd.DataFrame({\n",
    "        'Metric': ['Independence (M_ind)', 'Separation (M_sep)', 'Sufficiency (M_suff)'],\n",
    "        'Value': [round(m_ind, 4), round(m_sep, 4), round(m_suff, 4)]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08a213d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for bias diagnosis\n",
    "def bias_diagnosis(y_true, y_bias, sensitive):\n",
    "    male_mask = (sensitive == 'Male')\n",
    "    female_mask = (sensitive == 'Female')\n",
    "    error_rate_male = np.mean(y_true[male_mask] != y_bias[male_mask]) if male_mask.sum() > 0 else 0\n",
    "    error_rate_female = np.mean(y_true[female_mask] != y_bias[female_mask]) if female_mask.sum() > 0 else 0\n",
    "    return {'Male': round(error_rate_male, 4), 'Female': round(error_rate_female, 4)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154854ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for mitigation\n",
    "def apply_mitigation(gbm, X_val, y_pred_proba, sensitive, y_val, sensitive_val):\n",
    "    male_mask_val = (sensitive_val == 'Male')\n",
    "    female_mask_val = (sensitive_val == 'Female')\n",
    "    y_pred_proba_val = gbm.predict_proba(X_val)\n",
    "    best_thresholds = {'Male': 0.5, 'Female': 0.5}\n",
    "    best_score = float('inf')\n",
    "    thresholds = np.arange(0.3, 0.8, 0.1)\n",
    "    for t_male in thresholds:\n",
    "        for t_female in thresholds:\n",
    "            y_pred_val = np.zeros_like(y_val)\n",
    "            y_pred_val[male_mask_val] = (y_pred_proba_val[male_mask_val][:, 1] >= t_male).astype(int)\n",
    "            y_pred_val[female_mask_val] = (y_pred_proba_val[female_mask_val][:, 1] >= t_female).astype(int)\n",
    "            male_metrics = compute_metrics(y_val[male_mask_val], y_pred_val[male_mask_val], group_name='Male')\n",
    "            female_metrics = compute_metrics(y_val[female_mask_val], y_pred_val[female_mask_val], group_name='Female')\n",
    "            fairness = fairness_diagnosis(male_metrics, female_metrics)\n",
    "            sep_score = fairness['Value'][1]\n",
    "            acc = compute_metrics(y_val, y_pred_val)['Accuracy']\n",
    "            score = sep_score - acc\n",
    "            if score < best_score:\n",
    "                best_score = score\n",
    "                best_thresholds = {'Male': t_male, 'Female': t_female}\n",
    "    y_pred_mitigated = np.zeros_like(y_pred_proba[:, 1])\n",
    "    male_mask = (sensitive == 'Male')\n",
    "    female_mask = (sensitive == 'Female')\n",
    "    y_pred_mitigated[male_mask] = (y_pred_proba[male_mask][:, 1] >= best_thresholds['Male']).astype(int)\n",
    "    y_pred_mitigated[female_mask] = (y_pred_proba[female_mask][:, 1] >= best_thresholds['Female']).astype(int)\n",
    "    return y_pred_mitigated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5ddbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot metrics across flips (with overall line and mean ± SE annotations)\n",
    "def plot_metrics_across_flips(metrics_results, flip_percentages, metric_names, title_suffix, file_name):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    n_metrics = len(metric_names)\n",
    "    n_cols = 4\n",
    "    n_rows = (n_metrics + n_cols - 1) // n_cols\n",
    "    \n",
    "    for i, metric in enumerate(metric_names):\n",
    "        # Calculate mean and SE across seeds\n",
    "        male_values = np.array([[result['Male'][metric] for result in metrics] for metrics in metrics_results])\n",
    "        female_values = np.array([[result['Female'][metric] for result in metrics] for metrics in metrics_results])\n",
    "        overall_values = np.array([[result['Overall'][metric] for result in metrics] for metrics in metrics_results])\n",
    "        male_mean = np.mean(male_values, axis=0)\n",
    "        female_mean = np.mean(female_values, axis=0)\n",
    "        overall_mean = np.mean(overall_values, axis=0)\n",
    "        male_se = sem(male_values, axis=0)\n",
    "        female_se = sem(female_values, axis=0)\n",
    "        overall_se = sem(overall_values, axis=0)\n",
    "        \n",
    "        # Calculate overall mean and SE across all flip percentages\n",
    "        overall_male_mean = np.mean(male_mean)\n",
    "        overall_male_se = np.mean(male_se)\n",
    "        overall_female_mean = np.mean(female_mean)\n",
    "        overall_female_se = np.mean(female_se)\n",
    "        overall_overall_mean = np.mean(overall_mean)\n",
    "        overall_overall_se = np.mean(overall_se)\n",
    "        \n",
    "        plt.subplot(n_rows, n_cols, i+1)\n",
    "        plt.plot(flip_percentages, male_mean, 'o-', color='#1f77b4', label='Male Mean')\n",
    "        plt.fill_between(flip_percentages, male_mean - male_se, male_mean + male_se, color='#1f77b4', alpha=0.2, label='Male Mean ± SE')\n",
    "        plt.plot(flip_percentages, female_mean, 'o-', color='#ff7f0e', label='Female Mean')\n",
    "        plt.fill_between(flip_percentages, female_mean - female_se, female_mean + female_se, color='#ff7f0e', alpha=0.2, label='Female Mean ± SE')\n",
    "        plt.plot(flip_percentages, overall_mean, 'o-', color='#2ca02c', label='Overall Mean')\n",
    "        plt.fill_between(flip_percentages, overall_mean - overall_se, overall_mean + overall_se, color='#2ca02c', alpha=0.2, label='Overall Mean ± SE')\n",
    "        \n",
    "        # Add overall mean ± SE as text annotation\n",
    "        plt.text(0.02, 0.98, f'Male: {overall_male_mean:.4f} ± {overall_male_se:.4f}\\nFemale: {overall_female_mean:.4f} ± {overall_female_se:.4f}\\nOverall: {overall_overall_mean:.4f} ± {overall_overall_se:.4f}',\n",
    "                 transform=plt.gca().transAxes, fontsize=8, verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "        \n",
    "        plt.xlabel('Flip Percentage')\n",
    "        plt.ylabel(metric)\n",
    "        plt.title(f'{metric} vs Flip Percentage')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.ylim(0.0, 1.0)  # Set y-axis limits to 0.0 to 1.0\n",
    "    \n",
    "    plt.suptitle(f'Metrics vs Flip Percentage ({title_suffix})', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    with PdfPages(file_name) as pdf:\n",
    "        pdf.savefig()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d751a5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot VIP (Variable Importance) across seeds\n",
    "def plot_vip_across_seeds(feature_importance_results, flip_percentages, feature_names, exp_name):\n",
    "    # Calculate mean feature importance across seeds for each flip percentage\n",
    "    n_flips = len(flip_percentages)\n",
    "    n_features = len(feature_names)\n",
    "    \n",
    "    # Create a 3D array: seeds × flips × features\n",
    "    all_importance = np.array([[[fi[flip_idx][feature_idx] for feature_idx in range(n_features)] \n",
    "                              for flip_idx in range(n_flips)] \n",
    "                             for fi in feature_importance_results])\n",
    "    \n",
    "    # Calculate mean across seeds\n",
    "    mean_importance = np.mean(all_importance, axis=0)  # flips × features\n",
    "    \n",
    "    # Plot VIP for each flip percentage\n",
    "    plt.figure(figsize=(15, 20))\n",
    "    n_cols = 4\n",
    "    n_rows = (n_flips + n_cols - 1) // n_cols\n",
    "    \n",
    "    for flip_idx, flip_pct in enumerate(flip_percentages):\n",
    "        # Get feature importance for this flip percentage\n",
    "        importance = mean_importance[flip_idx]\n",
    "        \n",
    "        # Create a DataFrame for feature importance\n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Importance': importance\n",
    "        }).sort_values('Importance', ascending=True)\n",
    "        \n",
    "        # Plot VIP\n",
    "        plt.subplot(n_rows, n_cols, flip_idx+1)\n",
    "        plt.barh(range(len(feature_importance_df)), feature_importance_df['Importance'], align='center')\n",
    "        plt.yticks(range(len(feature_importance_df)), feature_importance_df['Feature'], fontsize=8)\n",
    "        plt.xlabel('Importance')\n",
    "        plt.title(f'Flip {flip_pct*100}%')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(f'Mean Variable Importance Across Seeds ({exp_name})', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    with PdfPages(f\"vip_mean_{exp_name}.pdf\") as pdf:\n",
    "        pdf.savefig()\n",
    "    plt.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82e23bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data (from local CSV file)\n",
    "mydf = pd.read_csv(\"dep.csv\")  \n",
    "mydf['StatusLabel'] = mydf['Status'].map({0: 'Not Depressed', 1: 'Depressed'})\n",
    "mydf['SuicideCat'] = mydf['Suicide'].map({0: 'Not Committed', 1: 'Committed Suicide'})\n",
    "features = ['JobStatus', 'Job', 'SleepDuration', 'DietaryHabits', 'Qualification', 'Suicide', 'Family']\n",
    "X = pd.get_dummies(mydf[features + ['Gender']], drop_first=False)\n",
    "ytrue = mydf['Status']\n",
    "sensitive_feature = mydf['Gender']\n",
    "feature_names = X.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0fa2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#..... If you have other scripts, you can just initiate from here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26af263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiments\n",
    "for seed_idx, seed in enumerate(seeds):\n",
    "    for exp in experiments:\n",
    "        all_metrics_before = []\n",
    "        all_metrics_after = []\n",
    "        all_fairness_before = []\n",
    "        all_fairness_after = []\n",
    "        all_bias_diagnosis = []\n",
    "        all_feature_importance = []  # Store feature importance for each flip\n",
    "        \n",
    "        # DataFrames to store results for this seed\n",
    "        metrics_before_df = []\n",
    "        metrics_after_df = []\n",
    "        fairness_before_df = []\n",
    "        fairness_after_df = []\n",
    "        bias_diagnosis_df = []\n",
    "        \n",
    "        for flip_pct in flip_percentages:\n",
    "            np.random.seed(seed)\n",
    "            mydf2 = mydf.copy()\n",
    "            mydf2['statusflip'] = mydf2['StatusLabel'].copy()\n",
    "            mask = (mydf2['Gender'] == 'Female') & (mydf2['statusflip'] == 'Not Depressed')\n",
    "            if mask.sum() > 0:\n",
    "                mydf2.loc[mask, 'statusflip'] = np.random.choice(\n",
    "                    ['Not Depressed', 'Depressed'],\n",
    "                    size=mask.sum(),\n",
    "                    p=[1 - flip_pct, flip_pct]\n",
    "                )\n",
    "            mydf2['statusflipbin'] = mydf2['statusflip'].map({'Not Depressed': 0, 'Depressed': 1})\n",
    "            ybias = mydf2['statusflipbin']\n",
    "            \n",
    "            X_temp, X_test, ytrue_temp, ytrue_test, ybias_temp, ybias_test, sensitive_temp, sensitive_test = train_test_split(\n",
    "                X, ytrue, ybias, sensitive_feature, test_size=0.2, random_state=seed, stratify=ytrue\n",
    "            )\n",
    "            X_train, X_val, ytrue_train, ytrue_val, ybias_train, ybias_val, sensitive_train, sensitive_val = train_test_split(\n",
    "                X_temp, ytrue_temp, ybias_temp, sensitive_temp, test_size=0.25, random_state=seed, stratify=ytrue_temp\n",
    "            )\n",
    "            \n",
    "            y_train = ytrue_train if exp['train'] == 'true' else ybias_train\n",
    "            y_val_for_mitigation = ytrue_val if exp['train'] == 'true' else ybias_val\n",
    "            y_test = ytrue_test if exp['test'] == 'true' else ybias_test\n",
    "            \n",
    "            ####################################################################\n",
    "            # Use LGBMClassifier with is_unbalance=True for imbalance\n",
    "            gbm = LGBMClassifier(random_state=seed, is_unbalance=True)\n",
    "            gbm.fit(X_train, y_train)\n",
    "            y_pred = gbm.predict(X_test)\n",
    "            y_pred_proba = gbm.predict_proba(X_test)\n",
    "            y_pred_mitigated = apply_mitigation(gbm, X_val, y_pred_proba, sensitive_test, y_val_for_mitigation, sensitive_val)\n",
    "\n",
    "            ####################################################################\n",
    "            # Store feature importance for this flip percentage\n",
    "            importance = gbm.feature_importances_  # Use feature importances from LGBMClassifier\n",
    "            all_feature_importance.append(importance)\n",
    "            ####################################################################\n",
    "\n",
    "            male_mask = (sensitive_test == 'Male')\n",
    "            female_mask = (sensitive_test == 'Female')\n",
    "            overall_metrics = compute_metrics(y_test, y_pred, group_name='Overall')\n",
    "            male_metrics = compute_metrics(y_test[male_mask], y_pred[male_mask], group_name='Male')\n",
    "            female_metrics = compute_metrics(y_test[female_mask], y_pred[female_mask], group_name='Female')\n",
    "            fairness = fairness_diagnosis(male_metrics, female_metrics)\n",
    "            \n",
    "            overall_metrics_mitigated = compute_metrics(y_test, y_pred_mitigated, group_name='Overall')\n",
    "            male_metrics_mitigated = compute_metrics(y_test[male_mask], y_pred_mitigated[male_mask], group_name='Male')\n",
    "            female_metrics_mitigated = compute_metrics(y_test[female_mask], y_pred_mitigated[female_mask], group_name='Female')\n",
    "            fairness_mitigated = fairness_diagnosis(male_metrics_mitigated, female_metrics_mitigated)\n",
    "            \n",
    "            bias_diag = bias_diagnosis(ytrue_test, ybias_test, sensitive_test)\n",
    "            \n",
    "            all_metrics_before.append({'Overall': overall_metrics, 'Male': male_metrics, 'Female': female_metrics})\n",
    "            all_metrics_after.append({'Overall': overall_metrics_mitigated, 'Male': male_metrics_mitigated, 'Female': female_metrics_mitigated})\n",
    "            all_fairness_before.append(fairness)\n",
    "            all_fairness_after.append(fairness_mitigated)\n",
    "            all_bias_diagnosis.append(bias_diag)\n",
    "            \n",
    "            # Store results for this seed in DataFrames\n",
    "            for group in ['Male', 'Female']:\n",
    "                metrics_before_df.append({\n",
    "                    'Flip Percentage': flip_pct * 100,\n",
    "                    'Group': group,\n",
    "                    **{metric: all_metrics_before[-1][group][metric] for metric in metric_names}\n",
    "                })\n",
    "                metrics_after_df.append({\n",
    "                    'Flip Percentage': flip_pct * 100,\n",
    "                    'Group': group,\n",
    "                    **{metric: all_metrics_after[-1][group][metric] for metric in metric_names}\n",
    "                })\n",
    "            fairness_before_df.append({\n",
    "                'Flip Percentage': flip_pct * 100,\n",
    "                **{row['Metric']: row['Value'] for _, row in fairness.iterrows()}\n",
    "            })\n",
    "            fairness_after_df.append({\n",
    "                'Flip Percentage': flip_pct * 100,\n",
    "                **{row['Metric']: row['Value'] for _, row in fairness_mitigated.iterrows()}\n",
    "            })\n",
    "            bias_diagnosis_df.append({\n",
    "                'Flip Percentage': flip_pct * 100,\n",
    "                'Male Error Rate': bias_diag['Male'],\n",
    "                'Female Error Rate': bias_diag['Female']\n",
    "            })\n",
    "        \n",
    "        all_experiment_results[exp['name']]['metrics_before'][seed_idx] = all_metrics_before\n",
    "        all_experiment_results[exp['name']]['metrics_after'][seed_idx] = all_metrics_after\n",
    "        all_experiment_results[exp['name']]['fairness_before'][seed_idx] = all_fairness_before\n",
    "        all_experiment_results[exp['name']]['fairness_after'][seed_idx] = all_fairness_after\n",
    "        all_experiment_results[exp['name']]['bias_diagnosis'][seed_idx] = all_bias_diagnosis\n",
    "        all_experiment_results[exp['name']]['feature_importance'][seed_idx] = all_feature_importance\n",
    "        \n",
    "        # Save seed-specific results to CSVs\n",
    "        pd.DataFrame(metrics_before_df).to_csv(f\"metrics_before_{exp['name']}_seed_{seed}.csv\", index=False)\n",
    "        pd.DataFrame(metrics_after_df).to_csv(f\"metrics_after_{exp['name']}_seed_{seed}.csv\", index=False)\n",
    "        pd.DataFrame(fairness_before_df).to_csv(f\"fairness_before_{exp['name']}_seed_{seed}.csv\", index=False)\n",
    "        pd.DataFrame(fairness_after_df).to_csv(f\"fairness_after_{exp['name']}_seed_{seed}.csv\", index=False)\n",
    "        pd.DataFrame(bias_diagnosis_df).to_csv(f\"bias_diagnosis_{exp['name']}_seed_{seed}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a77dce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate mean and standard error\n",
    "def calculate_stats(data_list, metric, group=None):\n",
    "    if group:\n",
    "        values = np.array([[m[group][metric] for m in metrics] for metrics in data_list])\n",
    "    else:\n",
    "        values = np.array([[df[df['Metric'] == metric]['Value'].values[0] for df in dfs] for dfs in data_list])\n",
    "    mean_values = np.mean(values, axis=0)\n",
    "    se_values = sem(values, axis=0)\n",
    "    return mean_values, se_values\n",
    "\n",
    "# Generate plots, print mean and SE, and save mean/SE CSVs\n",
    "for exp in experiments:\n",
    "    exp_name = exp['name']\n",
    "    results = all_experiment_results[exp_name]\n",
    "    \n",
    "    # Plot VIP across seeds for this experiment\n",
    "    plot_vip_across_seeds(results['feature_importance'], flip_percentages, feature_names, exp_name)\n",
    "    \n",
    "    # DataFrames for mean and SE\n",
    "    metrics_mean_se_df = []\n",
    "    fairness_mean_se_df = []\n",
    "    bias_diagnosis_mean_se_df = []\n",
    "    \n",
    "    # Print mean and SE for standard metrics and store in DataFrame\n",
    "    for group in ['Male', 'Female']:\n",
    "        for metric in metric_names:\n",
    "            mean_values_before, se_values_before = calculate_stats(results['metrics_before'], metric, group)\n",
    "            mean_values_after, se_values_after = calculate_stats(results['metrics_after'], metric, group)\n",
    "            print(f\"{exp_name} {group} {metric} - Before Mean: {mean_values_before}, SE: {se_values_before}\")\n",
    "            print(f\"{exp_name} {group} {metric} - After Mean: {mean_values_after}, SE: {se_values_after}\")\n",
    "            \n",
    "            for i, flip_pct in enumerate(flip_percentages):\n",
    "                metrics_mean_se_df.append({\n",
    "                    'Flip Percentage': flip_pct * 100,\n",
    "                    'Group': group,\n",
    "                    'Metric': metric,\n",
    "                    'Before Mean': mean_values_before[i],\n",
    "                    'Before SE': se_values_before[i],\n",
    "                    'After Mean': mean_values_after[i],\n",
    "                    'After SE': se_values_after[i]\n",
    "                })\n",
    "    \n",
    "    # Plot metrics (with overall line and mean ± SE annotations)\n",
    "    plot_metrics_across_flips(results['metrics_before'], [p*100 for p in flip_percentages], metric_names,\n",
    "                             f\"{exp_name} - Before Mitigation\", f\"metrics_vs_flip_{exp_name}_before.pdf\")\n",
    "    plot_metrics_across_flips(results['metrics_after'], [p*100 for p in flip_percentages], metric_names,\n",
    "                             f\"{exp_name} - After Mitigation\", f\"metrics_vs_flip_{exp_name}_after.pdf\")\n",
    "    \n",
    "    # Fairness metrics plot (with mean ± SE annotations)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for i, metric in enumerate(fairness_metrics):\n",
    "        before_values = np.array([[df[df['Metric'] == metric]['Value'].values[0] for df in dfs] for dfs in results['fairness_before']])\n",
    "        after_values = np.array([[df[df['Metric'] == metric]['Value'].values[0] for df in dfs] for dfs in results['fairness_after']])\n",
    "        before_mean = np.mean(before_values, axis=0)\n",
    "        after_mean = np.mean(after_values, axis=0)\n",
    "        before_se = sem(before_values, axis=0)\n",
    "        after_se = sem(after_values, axis=0)\n",
    "        \n",
    "        # Calculate overall mean and SE across all flip percentages\n",
    "        overall_before_mean = np.mean(before_mean)\n",
    "        overall_before_se = np.mean(before_se)\n",
    "        overall_after_mean = np.mean(after_mean)\n",
    "        overall_after_se = np.mean(after_se)\n",
    "        \n",
    "        print(f\"{exp_name} Fairness {metric} - Before Mean: {before_mean}, SE: {before_se}\")\n",
    "        print(f\"{exp_name} Fairness {metric} - After Mean: {after_mean}, SE: {after_se}\")\n",
    "        \n",
    "        for j, flip_pct in enumerate(flip_percentages):\n",
    "            fairness_mean_se_df.append({\n",
    "                'Flip Percentage': flip_pct * 100,\n",
    "                'Metric': metric,\n",
    "                'Before Mean': before_mean[j],\n",
    "                'Before SE': before_se[j],\n",
    "                'After Mean': after_mean[j],\n",
    "                'After SE': after_se[j]\n",
    "            })\n",
    "        \n",
    "        plt.subplot(2, 2, i+1)\n",
    "        plt.plot([p*100 for p in flip_percentages], before_mean, 'o-', color='#1f77b4', label='Before Mean')\n",
    "        plt.fill_between([p*100 for p in flip_percentages], before_mean - before_se, before_mean + before_se, color='#1f77b4', alpha=0.2, label='Before Mean ± SE')\n",
    "        plt.plot([p*100 for p in flip_percentages], after_mean, 'o-', color='#ff7f0e', label='After Mean')\n",
    "        plt.fill_between([p*100 for p in flip_percentages], after_mean - after_se, after_mean + after_se, color='#ff7f0e', alpha=0.2, label='After Mean ± SE')\n",
    "        \n",
    "        # Add overall mean ± SE as text annotation\n",
    "        plt.text(0.02, 0.98, f'Before: {overall_before_mean:.4f} ± {overall_before_se:.4f}\\nAfter: {overall_after_mean:.4f} ± {overall_after_se:.4f}',\n",
    "                 transform=plt.gca().transAxes, fontsize=8, verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "        \n",
    "        plt.xlabel('Flip Percentage')\n",
    "        plt.ylabel(metric)\n",
    "        plt.title(f'{metric} vs Flip Percentage')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.ylim(0.0, 1.0)  # Set y-axis limits to 0.0 to 1.0\n",
    "    \n",
    "    plt.suptitle(f'Fairness Metrics vs Flip Percentage ({exp_name})', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    with PdfPages(f\"fairness_vs_flip_{exp_name}.pdf\") as pdf:\n",
    "        pdf.savefig()\n",
    "    plt.close()\n",
    "    \n",
    "    # Bias diagnosis plot (with overall line and mean ± SE annotations)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    male_bias = np.array([[bias['Male'] for bias in biases] for biases in results['bias_diagnosis']])\n",
    "    female_bias = np.array([[bias['Female'] for bias in biases] for biases in results['bias_diagnosis']])\n",
    "    male_mean = np.mean(male_bias, axis=0)\n",
    "    female_mean = np.mean(female_bias, axis=0)\n",
    "    male_se = sem(male_bias, axis=0)\n",
    "    female_se = sem(female_bias, axis=0)\n",
    "    \n",
    "    # Calculate overall bias (mean of Male and Female error rates)\n",
    "    overall_bias = (male_bias + female_bias) / 2\n",
    "    overall_mean = np.mean(overall_bias, axis=0)\n",
    "    overall_se = sem(overall_bias, axis=0)\n",
    "    \n",
    "    # Calculate overall mean and SE across all flip percentages\n",
    "    overall_male_mean = np.mean(male_mean)\n",
    "    overall_male_se = np.mean(male_se)\n",
    "    overall_female_mean = np.mean(female_mean)\n",
    "    overall_female_se = np.mean(female_se)\n",
    "    overall_overall_mean = np.mean(overall_mean)\n",
    "    overall_overall_se = np.mean(overall_se)\n",
    "    \n",
    "    print(f\"{exp_name} Bias Diagnosis - Male Mean: {male_mean}, SE: {male_se}\")\n",
    "    print(f\"{exp_name} Bias Diagnosis - Female Mean: {female_mean}, SE: {female_se}\")\n",
    "    print(f\"{exp_name} Bias Diagnosis - Overall Mean: {overall_mean}, SE: {overall_se}\")\n",
    "    \n",
    "    for i, flip_pct in enumerate(flip_percentages):\n",
    "        bias_diagnosis_mean_se_df.append({\n",
    "            'Flip Percentage': flip_pct * 100,\n",
    "            'Male Mean': male_mean[i],\n",
    "            'Male SE': male_se[i],\n",
    "            'Female Mean': female_mean[i],\n",
    "            'Female SE': female_se[i],\n",
    "            'Overall Mean': overall_mean[i],\n",
    "            'Overall SE': overall_se[i]\n",
    "        })\n",
    "    \n",
    "    plt.plot([p*100 for p in flip_percentages], male_mean, 'o-', color='#1f77b4', label='Male Mean')\n",
    "    plt.fill_between([p*100 for p in flip_percentages], male_mean - male_se, male_mean + male_se, color='#1f77b4', alpha=0.2, label='Male Mean ± SE')\n",
    "    plt.plot([p*100 for p in flip_percentages], female_mean, 'o-', color='#ff7f0e', label='Female Mean')\n",
    "    plt.fill_between([p*100 for p in flip_percentages], female_mean - female_se, female_mean + female_se, color='#ff7f0e', alpha=0.2, label='Female Mean ± SE')\n",
    "    plt.plot([p*100 for p in flip_percentages], overall_mean, 'o-', color='#2ca02c', label='Overall Mean')\n",
    "    plt.fill_between([p*100 for p in flip_percentages], overall_mean - overall_se, overall_mean + overall_se, color='#2ca02c', alpha=0.2, label='Overall Mean ± SE')\n",
    "    \n",
    "    # Add overall mean ± SE as text annotation\n",
    "    plt.text(0.02, 0.98, f'Male: {overall_male_mean:.4f} ± {overall_male_se:.4f}\\nFemale: {overall_female_mean:.4f} ± {overall_female_se:.4f}\\nOverall: {overall_overall_mean:.4f} ± {overall_overall_se:.4f}',\n",
    "             transform=plt.gca().transAxes, fontsize=8, verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.xlabel('Flip Percentage')\n",
    "    plt.ylabel('Bias Error Rate')\n",
    "    plt.title(f'Bias Error Rate vs Flip Percentage ({exp_name})')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.ylim(0.0, 1.0)  # Set y-axis limits to 0.0 to 1.0\n",
    "    plt.tight_layout()\n",
    "    with PdfPages(f\"bias_error_vs_flip_{exp_name}.pdf\") as pdf:\n",
    "        pdf.savefig()\n",
    "    plt.close()\n",
    "    \n",
    "    # Save mean and SE results to CSVs\n",
    "    pd.DataFrame(metrics_mean_se_df).to_csv(f\"metrics_mean_se_{exp_name}.csv\", index=False)\n",
    "    pd.DataFrame(fairness_mean_se_df).to_csv(f\"fairness_mean_se_{exp_name}.csv\", index=False)\n",
    "    pd.DataFrame(bias_diagnosis_mean_se_df).to_csv(f\"bias_diagnosis_mean_se_{exp_name}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
